<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | The Whittington blog]]></title>
  <link href="http://elachys.github.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://elachys.github.com/"/>
  <updated>2013-01-25T14:19:29+00:00</updated>
  <id>http://elachys.github.com/</id>
  <author>
    <name><![CDATA[Chris Whittington]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Set pear to use beta channel]]></title>
    <link href="http://elachys.github.com/blog/2011/06/24/set-pear-to-use-beta-channel/"/>
    <updated>2011-06-24T09:09:16+01:00</updated>
    <id>http://elachys.github.com/blog/2011/06/24/set-pear-to-use-beta-channel</id>
    <content type="html"><![CDATA[<p>Sometimes you just need a little danger in your life.
If you have php pear installed, you can set it to install beta packages as follows:
<code>sudo pear config-set preferred_state beta</code></p>

<p>Afterwards, you can set it back by using the following command:
<code>sudo pear config-set preferred_state stable</code></p>

<p>Note: if you only need to do it for one package. You can always just do:
<code>pear -d preferred_state=beta install PHP_Beautifier</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Surpress error message in linux commands]]></title>
    <link href="http://elachys.github.com/blog/2011/06/24/surpress-error-message-in-linux-commands/"/>
    <updated>2011-06-24T09:06:51+01:00</updated>
    <id>http://elachys.github.com/blog/2011/06/24/surpress-error-message-in-linux-commands</id>
    <content type="html"><![CDATA[<p>When using a command like find, you can pipe all errors to dev null using the following syntax
<code>find / -name "somefilename" 2&gt;/dev/null</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install, index and test Sphinx (search engine)]]></title>
    <link href="http://elachys.github.com/blog/2011/05/30/sphinx-installation-usage/"/>
    <updated>2011-05-30T13:26:19+01:00</updated>
    <id>http://elachys.github.com/blog/2011/05/30/sphinx-installation-usage</id>
    <content type="html"><![CDATA[<h2>Installation</h2>

<p>download and extract the source:
<code>wget http://sphinxsearch.com/files/sphinx-2.0.1-beta.tar.gz
tar xvzf sphinx-2.0.1-beta.tar.gz
</code></p>

<p>Try and configure it (prefix is the path to install sphinx to)
<code>./configure --prefix=/usr/local/sphinx</code>
It will most likely error. In my case, i had to install the mysql-devel package, fix the problems and re-run configure until it works.</p>

<p><code>make</code>
At this point, i got the error:
<code>... line 512: exec: g++: not found</code>
I took this to mean i didn't have g++ installed...
<code>yum install gcc gcc-c++ make</code>
Then install
<code>make install</code></p>

<h2>Configuration</h2>

<p>If you've installed to the same location as me, make the config file in:
<code>/usr/local/sphinx/etc/sphinx.conf</code>
I like to use the following config file here. It gives me all the basic options for all sites on the server, but also allows me to drop a per-site config file into the folder that will also be included.
`</p>

<h1>!/bin/bash`</h1>

<p><code>cat &lt;&lt;-EOL
indexer
{
mem_limit = 32M
}</code></p>

<p>`searchd
{</p>

<h1>IP address to bind on</h1>

<h1>optional, default is 0.0.0.0 (ie. listen on all interfaces)</h1>

<p>#</p>

<h1>address                               = 127.0.0.1</h1>

<h1>address                               = 192.168.0.1</h1>

<h1>searchd TCP port number</h1>

<h1>mandatory, default is 3312</h1>

<p>port                            = 3312</p>

<h1>log file, searchd run info is logged here</h1>

<h1>optional, default is 'searchd.log'</h1>

<p>log                                     = /var/log/sphinx/searchd.log</p>

<h1>query log file, all search queries are logged here</h1>

<h1>optional, default is empty (do not log queries)</h1>

<p>query_log                       = /var/log/sphinx/query.log</p>

<h1>client read timeout, seconds</h1>

<h1>optional, default is 5</h1>

<p>read_timeout            = 5</p>

<h1>maximum amount of children to fork (concurrent searches to run)</h1>

<h1>optional, default is 0 (unlimited)</h1>

<p>max_children            = 30</p>

<h1>PID file, searchd process ID file name</h1>

<h1>mandatory</h1>

<p>pid_file                        = /var/log/sphinx/searchd.pid</p>

<h1>max amount of matches the daemon ever keeps in RAM, per-index</h1>

<h1>WARNING, THERE'S ALSO PER-QUERY LIMIT, SEE SetLimits() API CALL</h1>

<h1>default is 1000 (just like Google)</h1>

<p>max_matches                     = 100000</p>

<h1>seamless rotate, prevents rotate stalls if precaching huge datasets</h1>

<h1>optional, default is 1</h1>

<p>seamless_rotate         = 1</p>

<h1>whether to forcibly preopen all indexes on startup</h1>

<h1>optional, default is 0 (do not preopen)</h1>

<p>preopen_indexes         = 0</p>

<h1>whether to unlink .old index copies on succesful rotation.</h1>

<h1>optional, default is 1 (do unlink)</h1>

<p>unlink_old                      = 1
workers = threads
}
EOL</p>

<p>find . -name '*.include' -exec bash {} \;`</p>

<p>In this example config, we'll be reading from a MySQL database . Note: sphinx also supports Postgres. Our catalog is called catalog :-) you should name it something different for each different "thing" you'll be indexing.
`</p>

<h1>!/bin/bash</h1>

<p>cat &lt;&lt;-EOL
source catalog
{
type = mysql</p>

<p>sql_host = localhost
sql_user = dbuser
sql_pass = dbpassword
sql_db = dbtablename
sql_sock = /var/lib/mysql/mysql.sock
sql_port = 3306</p>

<h1>indexer query</h1>

<h1>document_id MUST be the very first field</h1>

<h1>document_id MUST be positive (non-zero, non-negative)</h1>

<h1>document_id MUST fit into 32 bits</h1>

<h1>document_id MUST be unique</h1>

<p>sql_query = SELECT id,field1 FROM tablename;</p>

<h1>sql_group_column = query</h1>

<h1>document info query</h1>

<h1>ONLY used by search utility to display document information</h1>

<h1>MUST be able to fetch document info by its id, therefore</h1>

<h1>MUST contain '$id' macro</h1>

<p>#
sql_query_info = SELECT * FROM tablename WHERE id=\$id</p>

<p>}</p>

<p>index catalog
{
source = catalog
path = /var/data/sphinx/catalog
morphology = stem_en</p>

<h1>min_word_len = 3</h1>

<h1>min_prefix_len = 0</h1>

<h1>min_infix_len = 3</h1>

<p>}</p>

<p>EOL
`</p>

<h2>Indexing &amp; Testing</h2>

<p>Move to your config directory
<code>cd /usr/local/sphinx/etc</code></p>

<p>Once everything is setup, you'll need to do an index of your data. Usually, the following command will do this:
<code>sudo /usr/local/sphinx/bin/indexer --all</code>
 You can then use the following to test (where your search term is "travel")
<code>/usr/local/sphinx/bin/search travel</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Finding large files]]></title>
    <link href="http://elachys.github.com/blog/2011/05/05/finding-large-files/"/>
    <updated>2011-05-05T08:44:55+01:00</updated>
    <id>http://elachys.github.com/blog/2011/05/05/finding-large-files</id>
    <content type="html"><![CDATA[<p>Easiest way to find the largest files / directories on a system</p>

<p><code>du -k | sort -n | perl -ne 'if ( /^(\d+)\s+(.*$)/){$l=log($1+.1);$m=int($l/log(1024)); printf  ("%6.1f\t%s\t%25s  %s\n",($1/(2**(10*$m))),(("K","M","G","T","P")[$m]),"*"x (1.5*$l),$2);}'</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improve Magento Performance]]></title>
    <link href="http://elachys.github.com/blog/2011/04/21/improve-magento-performance/"/>
    <updated>2011-04-21T03:24:15+01:00</updated>
    <id>http://elachys.github.com/blog/2011/04/21/improve-magento-performance</id>
    <content type="html"><![CDATA[<h2>Quick &amp; Easy Wins</h2>

<ul>
<li><p>Install Fooman Speedster</p></li>
<li><p>Enable Gzip Compression in .htaccess</p></li>
<li><p>HTTP KeepAlives enabled</p></li>
<li><p>Disable open_basedir</p></li>
</ul>


<h2>Mysql Settings</h2>

<p>my.cf should contain something like:
<code>key-buffer = 512M
max-allowed_packet = 64M
table-cache = 512
sort-buffer-size = 4M
read-buffer-size = 4M
read-rnd-buffer-size = 2M
myisam-sort-buffer-size 64M
tmp-table-size = 128M
thread-cache-size = 8
max-connections = 400
wait-timeout = 1800
connect_timeout = 120</code></p>

<h2>APC Caching</h2>

<p>Turn off the magento compiler module when using apc.stat=0
<code>apc.stat = 0
apc.shm_size = 512</code>
Modify app/etc/local.xml. Place the following just inside the <global> tag:
<code>&lt;cache&gt;
&lt;backend&gt;apc&lt;/backend&gt;
&lt;prefix&gt;alphanumeric&lt;/prefix&gt;
&lt;/cache&gt;</code></p>

<h2>Memory FS</h2>

<p>For when you have ram to burn. Dump the sessions and cache in ram (presuming you're using file based sessions).
<code>mount -t tmpfs -o size=256M,mode=0777 tmpfs /var/www/vhosts/yourmagentostore.com/httpdocs/var/cache
mount -t tmpfs -o size=64M,mode=0777 tmpfs /var/www/vhosts/yourmagentostore.com/httpdocs/var/session/</code></p>

<h2>Database Sessions</h2>

<p>Add the following to your app/etc/local.xml after </resources> (inside <global> )
<code>&lt;session_save&gt;&lt;![CDATA[db]]&gt;&lt;/session_save&gt;</code></p>
]]></content>
  </entry>
  
</feed>
